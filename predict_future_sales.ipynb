{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_future_sales.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svgkat/hse/blob/master/predict_future_sales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKYn7Swntung",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "from itertools import product\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import lightgbm as lgb\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import r2_score\n",
        "import gc \n",
        "%matplotlib inline \n",
        "pd.set_option('display.max_rows',600)\n",
        "pd.set_option('display.max_columns',50)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def downcast_dtypes(df):\n",
        "    '''\n",
        "        Changes column types in the dataframe: \n",
        "                \n",
        "                `float64` type to `float32`\n",
        "                `int64`   type to `int32`\n",
        "    '''\n",
        "    \n",
        "    # Select columns to downcast\n",
        "    int8_cols = [col for col in df.columns if col in ['shop_id','date_block_num','item_category_id'] ]\n",
        "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
        "    int_cols =   [c for c in df if df[c].dtype == \"int64\"] \n",
        "    int_cols = list(set(int_cols) - set(int8_cols))\n",
        "    # Downcast\n",
        "    df[float_cols] = df[float_cols].astype(np.float16)\n",
        "    df[int_cols]   = df[int_cols].astype(np.int16)\n",
        "    df[int8_cols] = df[int8_cols].astype(np.int8)\n",
        "    \n",
        "    return df\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"svgkat\"\n",
        "os.environ['KAGGLE_KEY'] = \"85ffe6ac1c5ea0226c2f7a857e9dc63e\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ACaGAjst-uH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edb1323e-14ef-478f-e8c8-c1d7890719f6"
      },
      "source": [
        "sales = pd.read_csv('/content/predict_future_sales/sales_train.csv.gz')\n",
        "shops = pd.read_csv('/content/predict_future_sales/shops.csv')\n",
        "items = pd.read_csv('/content/predict_future_sales/items.csv')\n",
        "item_cats = pd.read_csv('/content/predict_future_sales/item_categories.csv')\n",
        "test = pd.read_csv('/content/predict_future_sales/test.csv.gz')    \n",
        "\n",
        "sales = downcast_dtypes(sales)\n",
        "shops = downcast_dtypes(shops)\n",
        "items= downcast_dtypes(items)\n",
        "item_cats = downcast_dtypes(item_cats)\n",
        "test = downcast_dtypes(test)\n",
        "\n",
        "test.drop(columns='ID',inplace=True)\n",
        "test['date_block_num'] = sales.date_block_num.max() + 1\n",
        "\n",
        "#adding some month and year features\n",
        "sales.date=sales.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\n",
        "sales['month'] = sales.date.dt.month\n",
        "sales['year'] = sales.date.dt.year\n",
        "sales_month_year = sales[['date_block_num','month','year']].drop_duplicates() \n",
        "sales_month_year = sales_month_year.reset_index().drop(columns='index',axis=1)\n",
        "##adding the month year row corresponding to test data\n",
        "sales_month_year.loc[34] = [34]+[11]+[2015]\n",
        "sales_month_year = pd.get_dummies(sales_month_year,columns=['year'])\n",
        "sales_month_year = downcast_dtypes(sales_month_year)\n",
        "\n",
        "# Create \"grid\" with columns\n",
        "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
        "\n",
        "# For every month we create a grid from all shops/items combinations from that month\n",
        "grid = [] \n",
        "for block_num in sales['date_block_num'].unique():\n",
        "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
        "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
        "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
        "\n",
        "# Turn the grid into a dataframe\n",
        "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
        "\n",
        "#Adding the test data to ensure that the test data is also part of the \n",
        "#feature engineering process\n",
        "grid = pd.concat([grid, test])\n",
        "\n",
        "# Groupby data to get shop-item-month aggregates\n",
        "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
        "# Fix column names\n",
        "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
        "# Join it to the grid\n",
        "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
        "\n",
        "# Same as above but with shop-month aggregates\n",
        "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
        "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
        "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
        "\n",
        "# Same as above but with item-month aggregates\n",
        "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
        "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
        "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
        "\n",
        "# Downcast dtypes from 64 to 32 bit to save memory\n",
        "all_data = downcast_dtypes(all_data)\n",
        "del grid, gb \n",
        "gc.collect()\n",
        "\n",
        "## Creating return features ##\n",
        "sales_ret = sales[sales.item_cnt_day < 0]\n",
        "\n",
        "## return numbers for shop-month\n",
        "gb = sales_ret.groupby(by=['shop_id','date_block_num'],as_index=False).agg({'item_cnt_day':{'trg_ret_shop_per_mth':'sum'}})\n",
        "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
        "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
        "\n",
        "del gb\n",
        "gc.collect()\n",
        "\n",
        "##return numbers for item-month\n",
        "\n",
        "gb = sales_ret.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'trg_ret_item_mth':'sum'}})\n",
        "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
        "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
        "\n",
        "del gb\n",
        "gc.collect()\n",
        "\n",
        "##return numbers for shop-item-month\n",
        "\n",
        "gb = sales_ret.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'trg_ret_shop_item_mth':'sum'}})\n",
        "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
        "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
        "\n",
        "del gb,sales_ret\n",
        "gc.collect()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehhOdCwBuE2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "74a1e5cc-1823-4755-90f6-9a63fed549e1"
      },
      "source": [
        "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
        "\n",
        "#shift_range = [1, 2, 3, 4, 5, 12]\n",
        "shift_range = [1,2,3,6,9,12]\n",
        "for month_shift in shift_range:\n",
        "\tprint('Processing:',month_shift)\n",
        "\ttrain_shift = all_data[index_cols + cols_to_rename].copy()\n",
        "\ttrain_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
        "\tfoo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
        "\ttrain_shift = train_shift.rename(columns=foo)\n",
        "\tall_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
        "\n",
        "del train_shift\n",
        "gc.collect()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing: 1\n",
            "Processing: 2\n",
            "Processing: 3\n",
            "Processing: 6\n",
            "Processing: 9\n",
            "Processing: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnp2aV0NuYy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = all_data[all_data['date_block_num'] >= 12] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIj5Sw8Buub2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f6b0f19-d488-4b8c-cacb-dcce3ae4d5a3"
      },
      "source": [
        "# List of all lagged features\n",
        "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
        "# We will drop these at fitting stage\n",
        "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
        "\n",
        "print('The number of items in the dataset:',items.item_name.count())"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of items in the dataset: 22170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRWj3A0au2T-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "eeefe1d5-7b52-46e1-86a6-64d54affe6c4"
      },
      "source": [
        "## 22170 is a huge number to vectorize. I' am taking a concious decision to \n",
        "## not vectorize the item names. \n",
        "\n",
        "## Note: Having more features is crashing the coursera runtime/colab runtime\n",
        "## so selecting only the top 10 features using the max_features parameter\n",
        "\n",
        "vectorizer_item_cat = TfidfVectorizer(min_df=3,max_features=5)\n",
        "X = vectorizer_item_cat.fit_transform(item_cats.item_category_name)\n",
        "print('Count of features:',len(vectorizer_item_cat.get_feature_names()))\n",
        "print(vectorizer_item_cat.get_feature_names())\n",
        "df_item_cat_feat = pd.DataFrame(X.toarray())\n",
        "for ind in range(len(vectorizer_item_cat.get_feature_names())):\n",
        "  item_cats['tfidf_feat_itemcat_'+str(ind)] = df_item_cat_feat[ind]\n",
        "del vectorizer_item_cat,X\n",
        "df_item_cat_feat = downcast_dtypes(df_item_cat_feat)\n",
        "gc.collect()\n",
        "\n",
        "#merge the items and item_cats data frame so as to join with all_data\n",
        "item_item_cats = pd.merge(items.drop(columns=['item_name']),item_cats.drop(columns=['item_category_name']),how='left',on='item_category_id')\n",
        "item_item_cats = downcast_dtypes(item_item_cats)\n",
        "del df_item_cat_feat\n",
        "gc.collect()\n",
        "\n",
        "all_data = pd.merge(all_data,item_item_cats,how='left',on='item_id')\n",
        "all_data = downcast_dtypes(all_data)\n",
        "del item_item_cats\n",
        "gc.collect()\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of features: 5\n",
            "['игры', 'книги', 'консоли', 'подарки', 'цифра']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHviaVW4u8MN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e9644821-6f4a-4e6c-b7cc-1086ee14e654"
      },
      "source": [
        "##generate tfidf features from shop and add it to all_data \n",
        "vectorizer = TfidfVectorizer(min_df=2,max_features=5)\n",
        "X = vectorizer.fit_transform(shops.shop_name)\n",
        "print('Count of features:',len(vectorizer.get_feature_names()))\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "df_shop_features = pd.DataFrame(X.toarray())\n",
        "for ind in range(len(vectorizer.get_feature_names())):\n",
        "  shops['tfidf_feat_shop_'+str(ind)] = df_shop_features[ind]\n",
        "\n",
        "## Merge the shop features to all_data\n",
        "all_data = pd.merge(all_data,shops.drop(columns=['shop_name']),how='left',on='shop_id')\n",
        "all_data = downcast_dtypes(all_data)\n",
        "del vectorizer,df_shop_features, X\n",
        "gc.collect()\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of features: 5\n",
            "['мега', 'москва', 'тк', 'трц', 'тц']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tDep9EovCVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Adding the item_category_id to the drop cols list so that it \n",
        "## it is not part of the training\n",
        "to_drop_cols = to_drop_cols + ['item_category_id']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueGfRjESvFCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Add the month and year features from sales_month_year\n",
        "all_data=pd.merge(all_data,sales_month_year,how='left',on='date_block_num')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTHv4ovWvHB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Prepping the data for training\n",
        "dates = all_data['date_block_num']\n",
        "test_block = dates.unique()[-1]\n",
        "\n",
        "dates_train = dates[dates < test_block]\n",
        "dates_test = dates[dates == test_block]\n",
        "\n",
        "X_train = all_data.loc[dates < test_block].drop(to_drop_cols,axis = 1)\n",
        "X_test = all_data.loc[dates == test_block].drop(to_drop_cols,axis=1)\n",
        "\n",
        "y_train = all_data.loc[dates <  test_block, 'target'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8amy-m3vKlt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "596c57af-642b-4e5d-f571-5ea16c4db60e"
      },
      "source": [
        "del sales\n",
        "gc.collect()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "308"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yginr4_vRLv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67644257-cee6-42d5-d240-dd4008c50216"
      },
      "source": [
        "#First level models\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train.values,y_train)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E37ds29FvYke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_lr = lin_reg.predict(X_test.values).clip(0,20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bwQKGz6vi2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgb_params = {\n",
        "               'feature_fraction': 0.75,\n",
        "               'metric': 'rmse',\n",
        "               'nthread':1, \n",
        "               'min_data_in_leaf': 2**7, \n",
        "               'bagging_fraction': 0.75, \n",
        "               'learning_rate': 0.03, \n",
        "               'objective': 'mse', \n",
        "               'bagging_seed': 2**7, \n",
        "               'num_leaves': 2**7,\n",
        "               'bagging_freq':1,\n",
        "               'verbose':0 \n",
        "              }\n",
        "\n",
        "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDpJsiXdvk4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Clipping the values withing the range (0,20) as observed in the kernel submitted \n",
        "## by Denis Larionov\n",
        "pred_lgb = model.predict(X_test).clip(0,20)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYauty5DwWL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##test meta features \n",
        "X_test_level2 = np.c_[pred_lr, pred_lgb] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4PI9OIswYmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Train meta features\n",
        "dates_train_level2 = dates_train[dates_train.isin([ 28, 29, 30, 31, 32,33])]\n",
        "\n",
        "# That is how we get target for the 2nd level dataset\n",
        "y_train_level2 = y_train[dates_train.isin([ 28, 29, 30, 31, 32, 33])]\n",
        "\n",
        "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FntlySuhwa5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f66db393-084d-4985-95a1-45cb88b33db4"
      },
      "source": [
        "# Now fill `X_train_level2` with metafeatures\n",
        "for cur_block_num in [ 28, 29, 30, 31, 32, 33]:\n",
        "    \n",
        "    print(cur_block_num)\n",
        "    \n",
        "    '''\n",
        "        1. Split `X_train` into parts\n",
        "           Remember, that corresponding dates are stored in `dates_train` \n",
        "        2. Fit linear regression \n",
        "        3. Fit LightGBM and put predictions          \n",
        "        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
        "           You can use `dates_train_level2` for it\n",
        "           Make sure the order of the meta-features is the same as in `X_test_level2`\n",
        "    '''      \n",
        "    #  YOUR CODE GOES HERE\n",
        "    X_train_cur_block_num  = all_data.loc[dates <  cur_block_num].drop(to_drop_cols, axis=1)\n",
        "    X_test_cur_block_num = all_data.loc[dates ==  cur_block_num].drop(to_drop_cols, axis=1)\n",
        "    \n",
        "    y_train_cur_block_num = all_data.loc[dates <  cur_block_num,'target'].values\n",
        "    y_test_cur_block_num = all_data.loc[dates == cur_block_num, 'target'].values\n",
        "    \n",
        "    lrg = LinearRegression()\n",
        "    lrg.fit(X_train_cur_block_num.values, y_train_cur_block_num)\n",
        "    pred_lr_cur_block_num = lrg.predict(X_test_cur_block_num.values).clip(0,20)\n",
        "    \n",
        "    lgb_model = lgb.train(lgb_params, lgb.Dataset(X_train_cur_block_num, label=y_train_cur_block_num), 100)\n",
        "    pred_lgb_cur_block_num = lgb_model.predict(X_test_cur_block_num).clip(0,20)\n",
        "    \n",
        "    X_train_level2[dates_train_level2.isin([cur_block_num])] = np.c_[pred_lr_cur_block_num, pred_lgb_cur_block_num] "
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8rwF3Owi8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_level2 = np.array(y_train_level2, dtype = np.float64)\n",
        "X_train_level2= np.array(X_train_level2, dtype = np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7GBz4xX0knW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alphas_to_try = np.linspace(0, 1, 1000)\n",
        "r2_scores = np.array([r2_score(y_train_level2, np.dot(X_train_level2, [alpha, 1 - alpha])) for alpha in (alphas_to_try)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vod832w0p88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4499ab3b-262b-40fe-e908-195d5b34300b"
      },
      "source": [
        "print(r2_scores.argmax())\n",
        "best_alpha = alphas_to_try[r2_scores.argmax()]# YOUR CODE GOES HERE\n",
        "r2_train_simple_mix = r2_scores[r2_scores.argmax()]# YOUR CODE GOES HERE\n",
        "print('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, r2_train_simple_mix))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "142\n",
            "Best alpha: 0.142142; Corresponding r2 score on train: 0.083854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u2jdh2i0xCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_preds = np.dot(X_test_level2,[best_alpha,(1-best_alpha)]).clip(0,20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOK9jppS00Fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test['item_cnt_month'] = test_preds\n",
        "test['ID'] = test.index.to_list()\n",
        "test[['ID','item_cnt_month']].to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyrzuTgQ02Rs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6cee9864-76e7-4239-accf-72039a12ae9b"
      },
      "source": [
        "!kaggle competitions submit -c competitive-data-science-predict-future-sales -f submission.csv -m \"convex mix with neg return items\""
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 5.35M/5.35M [00:00<00:00, 15.4MB/s]\n",
            "Successfully submitted to Predict Future Sales"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}